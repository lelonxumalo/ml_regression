{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit_label</th>\n",
       "      <th>fruit_name</th>\n",
       "      <th>fruit_subtype</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruit_label fruit_name fruit_subtype  mass  width  height  color_score\n",
       "0            1      apple  granny_smith   192    8.4     7.3         0.55\n",
       "1            1      apple  granny_smith   180    8.0     6.8         0.59\n",
       "2            1      apple  granny_smith   176    7.4     7.2         0.60\n",
       "3            2   mandarin      mandarin    86    6.2     4.7         0.80\n",
       "4            2   mandarin      mandarin    84    6.0     4.6         0.79"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = pd.read_table('fruit_data_with_colors.txt')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "fruits.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.95\n",
      "Accuracy of K-NN classifier on test set: 1.00\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test KNN classifier on previously unseen instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted fruit type for  [[5.5, 2.2, 10, 0.7]]  is  mandarin\n"
     ]
    }
   ],
   "source": [
    "example_fruit = [[5.5, 2.2, 10, 0.70]] # the data in the brackets correspond to ['height', 'width', 'mass', 'color_score']\n",
    "example_fruit_scaled = scaler.transform(example_fruit)\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "          target_names_fruits[knn.predict(example_fruit_scaled)[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th KNN classifier predicts this to be a mandarin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "linear model intercept: -1728.1306725811305\n",
      "linear model coeff:\n",
      "[ 1.62e-03 -9.43e+01  1.36e+01 -3.13e+01 -8.15e-02 -1.69e+01 -2.43e-03\n",
      "  1.53e+00 -1.39e-02 -7.72e+00  2.28e+01 -5.66e+00  9.35e+00  2.07e-01\n",
      " -7.43e+00  9.66e-03  4.38e-03  4.80e-03 -4.46e+00 -1.61e+01  8.83e+00\n",
      " -5.07e-01 -1.42e+00  8.18e+00 -3.87e+00 -3.54e+00  4.49e+00  9.31e+00\n",
      "  1.74e+02  1.18e+01  1.51e+02 -3.30e+02 -1.35e+02  6.95e-01 -2.38e+01\n",
      "  2.77e+00  3.82e-01  4.39e+00 -1.06e+01 -4.92e-03  4.14e+01 -1.16e-03\n",
      "  1.19e+00  1.75e+00 -3.68e+00  1.60e+00 -8.42e+00 -3.80e+01  4.74e+01\n",
      " -2.51e+01 -2.88e-01 -3.66e+01  1.90e+01 -4.53e+01  6.83e+02  1.04e+02\n",
      " -3.29e+02 -3.14e+01  2.74e+01  5.12e+00  6.92e+01  1.98e-02 -6.12e-01\n",
      "  2.65e+01  1.01e+01 -1.59e+00  2.24e+00  7.38e+00 -3.14e+01 -9.78e-05\n",
      "  5.02e-05 -3.48e-04 -2.50e-04 -5.27e-01 -5.17e-01 -4.10e-01  1.16e-01\n",
      "  1.46e+00 -3.04e-01  2.44e+00 -3.66e+01  1.41e-01  2.89e-01  1.77e+01\n",
      "  5.97e-01  1.98e+00 -1.36e-01 -1.85e+00]\n",
      "R-squared score (training): 0.673\n",
      "R-squared score (test): 0.496\n"
     ]
    }
   ],
   "source": [
    "crime = load_crime_dataset()\n",
    "(X_crime, y_crime) = load_crime_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression adds a penalty term (a practice called Regularization) in order to prevent overfitting by restricting the model. This it does by reducing the model's complexity. The form of regularization that Ridge uses is called **L2 regularization**: minimizes the sum of squared *b* entries. The influence of L2 is moderated by alpha parameter. Higher alpha means more regularization and simpler models. You can change the alpha in the cell below, I have set it to 20. In scikitlearn the default is 1.0. If you set it to 0, the output is the same as OLS above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge regression linear model intercept: -3352.4230358459454\n",
      "ridge regression linear model coeff:\n",
      "[ 1.95e-03  2.19e+01  9.56e+00 -3.59e+01  6.36e+00 -1.97e+01 -2.81e-03\n",
      "  1.66e+00 -6.61e-03 -6.95e+00  1.72e+01 -5.63e+00  8.84e+00  6.79e-01\n",
      " -7.34e+00  6.70e-03  9.79e-04  5.01e-03 -4.90e+00 -1.79e+01  9.18e+00\n",
      " -1.24e+00  1.22e+00  1.03e+01 -3.78e+00 -3.73e+00  4.75e+00  8.43e+00\n",
      "  3.09e+01  1.19e+01 -2.05e+00 -3.82e+01  1.85e+01  1.53e+00 -2.20e+01\n",
      "  2.46e+00  3.29e-01  4.02e+00 -1.13e+01 -4.70e-03  4.27e+01 -1.23e-03\n",
      "  1.41e+00  9.35e-01 -3.00e+00  1.12e+00 -1.82e+01 -1.55e+01  2.42e+01\n",
      " -1.32e+01 -4.20e-01 -3.60e+01  1.30e+01 -2.81e+01  4.39e+01  3.87e+01\n",
      " -6.46e+01 -1.64e+01  2.90e+01  4.15e+00  5.34e+01  1.99e-02 -5.47e-01\n",
      "  1.24e+01  1.04e+01 -1.57e+00  3.16e+00  8.78e+00 -2.95e+01 -2.33e-04\n",
      "  3.14e-04 -4.13e-04 -1.80e-04 -5.74e-01 -5.18e-01 -4.21e-01  1.53e-01\n",
      "  1.33e+00  3.85e+00  3.03e+00 -3.78e+01  1.38e-01  3.08e-01  1.57e+01\n",
      "  3.31e-01  3.36e+00  1.61e-01 -2.68e+00]\n",
      "R-squared score (training): 0.671\n",
      "R-squared score (test): 0.494\n",
      "Number of non-zero features: 88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "crime = load_crime_dataset()\n",
    "(X_crime, y_crime) = load_crime_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results arent that impressive compared to OLS. We need to do feature preprocessing to transform input features so the L2 is applied fairly across features. We use MinMax scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge regression linear model intercept: 933.3906385044152\n",
      "ridge regression linear model coeff:\n",
      "[  88.69   16.49  -50.3   -82.91  -65.9    -2.28   87.74  150.95   18.88\n",
      "  -31.06  -43.14 -189.44   -4.53  107.98  -76.53    2.86   34.95   90.14\n",
      "   52.46  -62.11  115.02    2.67    6.94   -5.67 -101.55  -36.91   -8.71\n",
      "   29.12  171.26   99.37   75.07  123.64   95.24 -330.61 -442.3  -284.5\n",
      " -258.37   17.66 -101.71  110.65  523.14   24.82    4.87  -30.47   -3.52\n",
      "   50.58   10.85   18.28   44.11   58.34   67.09  -57.94  116.14   53.81\n",
      "   49.02   -7.62   55.14  -52.09  123.39   77.13   45.5   184.91  -91.36\n",
      "    1.08  234.09   10.39   94.72  167.92  -25.14   -1.18   14.6    36.77\n",
      "   53.2   -78.86   -5.9    26.05  115.15   68.74   68.29   16.53  -97.91\n",
      "  205.2    75.97   61.38  -79.83   67.27   95.67  -11.88]\n",
      "R-squared score (training): 0.615\n",
      "R-squared score (test): 0.599\n",
      "Number of non-zero features: 88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed model shows improved R-squared for our Ridge. One downside of feature normalization is that the result can be hard to intepret. Below we allow for a menu of alpha parameters. The best model is achieved with an alpha of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 88, r-squared training: 0.67, r-squared test: 0.50\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.66, r-squared test: 0.56\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.63, r-squared test: 0.59\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 88, r-squared training: 0.61, r-squared test: 0.60\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 86, r-squared training: 0.58, r-squared test: 0.58\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.55, r-squared test: 0.55\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 84, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.480193332104544e-18\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Ridge, Lasso adds a regularization parameter to reduce overfitting. Lasso, however, uses the L1 regularization paramter as a penalty when minimizing the RSS. L1 regularization minimizes the sum of the absolute values of the coefficients instead the sum of squared values of the coefficients. Is relatively easier to intepret. When we have many small/mdeium sized effects, use Ridge; when we have only a few variables with medium/large effects use lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "from adspy_shared_utilities import load_crime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = load_crime_dataset()\n",
    "(X_crime, y_crime) = load_crime_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "lasso regression linear model intercept: 1186.6120619985809\n",
      "lasso regression linear model coeff:\n",
      "[    0.       0.      -0.    -168.18    -0.      -0.       0.     119.69\n",
      "     0.      -0.       0.    -169.68    -0.       0.      -0.       0.\n",
      "     0.       0.      -0.      -0.       0.      -0.       0.       0.\n",
      "   -57.53    -0.      -0.       0.     259.33    -0.       0.       0.\n",
      "     0.      -0.   -1188.74    -0.      -0.      -0.    -231.42     0.\n",
      "  1488.37     0.      -0.      -0.      -0.       0.       0.       0.\n",
      "     0.       0.      -0.       0.      20.14     0.       0.       0.\n",
      "     0.       0.     339.04     0.       0.     459.54    -0.       0.\n",
      "   122.69    -0.      91.41     0.      -0.       0.       0.      73.14\n",
      "     0.      -0.       0.       0.      86.36     0.       0.       0.\n",
      "  -104.57   264.93     0.      23.45   -49.39     0.       5.2      0.  ]\n",
      "Non-zero features: 20\n",
      "R-squared score (training): 0.631\n",
      "R-squared score (test): 0.624\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tpctKidsBornNevrMarr, 1488.365\n",
      "\tpctKids2Par, -1188.740\n",
      "\thouseVacant, 459.538\n",
      "\tpctPopDenseHous, 339.045\n",
      "\tpersEmergShelt, 264.932\n",
      "\tpctMaleDivorc, 259.329\n",
      "\tpctWorkMom-18, -231.423\n",
      "\tpctWdiv, -169.676\n",
      "\tpct12-29, -168.183\n",
      "\tpctVacantBoarded, 122.692\n",
      "\tpctUrban, 119.694\n",
      "\tmedOwnCostPctWO, -104.571\n",
      "\tmedYrHousBuilt, 91.412\n",
      "\trentQrange , 86.356\n",
      "\townHousUperQ, 73.144\n",
      "\tpctEmployMfg, -57.530\n",
      "\tpctBornStateResid, -49.394\n",
      "\tpctForeignBorn, 23.449\n",
      "\tpctLargHousFam, 20.144\n",
      "\tpctSameCounty-5, 5.198\n"
     ]
    }
   ],
   "source": [
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the superior test score of 0.624 compared with Ridge? Let us now add various alpha parameters to see which level is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 35, r-squared training: 0.65, r-squared test: 0.58\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 25, r-squared training: 0.64, r-squared test: 0.60\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 20, r-squared training: 0.63, r-squared test: 0.62\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 17, r-squared training: 0.62, r-squared test: 0.63\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 12, r-squared training: 0.60, r-squared test: 0.61\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 6, r-squared training: 0.57, r-squared test: 0.58\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 2, r-squared training: 0.51, r-squared test: 0.50\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 1, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\nparameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 2 has the highest r-squared test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model coeff (w): [ 4.42  6.    0.53 10.24  6.55 -2.02 -0.32]\n",
      "linear model intercept (b): 1.543\n",
      "R-squared score (training): 0.722\n",
      "R-squared score (test): 0.722\n",
      "\n",
      "Now we transform the original input data to add\n",
      "polynomial features up to degree 2 (quadratic)\n",
      "\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 3.41e-12  1.66e+01  2.67e+01 -2.21e+01  1.24e+01  6.93e+00  1.05e+00\n",
      "  3.71e+00 -1.34e+01 -5.73e+00  1.62e+00  3.66e+00  5.05e+00 -1.46e+00\n",
      "  1.95e+00 -1.51e+01  4.87e+00 -2.97e+00 -7.78e+00  5.15e+00 -4.65e+00\n",
      "  1.84e+01 -2.22e+00  2.17e+00 -1.28e+00  1.88e+00  1.53e-01  5.62e-01\n",
      " -8.92e-01 -2.18e+00  1.38e+00 -4.90e+00 -2.24e+00  1.38e+00 -5.52e-01\n",
      " -1.09e+00]\n",
      "(poly deg 2) linear model intercept (b): -3.206\n",
      "(poly deg 2) R-squared score (training): 0.969\n",
      "(poly deg 2) R-squared score (test): 0.805\n",
      "\n",
      "\n",
      "Addition of many polynomial features often leads to\n",
      "overfitting, so we often use polynomial features in combination\n",
      "with regression that has a regularization penalty, like ridge\n",
      "regression.\n",
      "\n",
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[ 0.    2.23  4.73 -3.15  3.86  1.61 -0.77 -0.15 -1.75  1.6   1.37  2.52\n",
      "  2.72  0.49 -1.94 -1.63  1.51  0.89  0.26  2.05 -1.93  3.62 -0.72  0.63\n",
      " -3.16  1.29  3.55  1.73  0.94 -0.51  1.7  -1.98  1.81 -0.22  2.88 -0.89]\n",
      "(poly deg 2 + ridge) linear model intercept (b): 5.418\n",
      "(poly deg 2 + ridge) R-squared score (training): 0.826\n",
      "(poly deg 2 + ridge) R-squared score (test): 0.825\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\npolynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\noverfitting, so we often use polynomial features in combination\\nwith regression that has a regularization penalty, like ridge\\nregression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
